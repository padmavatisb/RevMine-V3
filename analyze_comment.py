import os
import requests
import json

def run_analysis():
    # 1. Get comment from GitHub Environment
    comment = os.getenv("COMMENT_BODY", "No comment found.")
    
    # 2. Construct the prompt for DeepSeek-R1
    # We ask for a structured table to make it look professional
    prompt = (
        f"Analyze the following Pull Request comment for technical depth and logic. "
        f"Return a Markdown table with 'Category', 'Observation', and 'Severity' (Low/Medium/High).\n"
        f"Comment: {comment}"
    )

    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "deepseek-r1:7b",
        "prompt": prompt,
        "stream": False
    }

    try:
        # 3. Call your local Ollama
        response = requests.post(url, json=payload, timeout=120)
        ai_response = response.json().get('response', 'AI Error: No response content.')

        # 4. Save the report
        with open("research_report.md", "w", encoding="utf-8") as f:
            f.write(f"### üß™ Automated Research Analysis\n\n{ai_response}")
            f.write("\n\n> _Generated by DeepSeek-R1 via Local Self-Hosted Runner_")

    except Exception as e:
        with open("research_report.md", "w") as f:
            f.write(f"‚ùå **Error:** Could not connect to local AI engine. {str(e)}")

if __name__ == "__main__":
    run_analysis()